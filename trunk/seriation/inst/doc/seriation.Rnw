\documentclass[10pt,a4paper,fleqn]{article}

\usepackage{a4wide}
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{6pt plus 2pt minus 1pt}

\usepackage[round,longnamesfirst]{natbib}
\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amsfonts}

\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\func}[1]{\mbox{\texttt{#1()}}}
\newcommand{\code}[1]{\mbox{\texttt{#1}}}
\newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\samp}[1]{`\mbox{\texttt{#1}}'}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\usepackage{Sweave}
%% \VignetteIndexEntry{Introduction to seriation}



\begin{document}


<<echo=FALSE>>=
options(width = 75)
### for sampling
set.seed(1234)
@

\title{Introduction to the package \pkg{seriation}}
\author{Michael Hahsler and Christian Buchta and Kurt Hornik}
\maketitle
\sloppy

\abstract{}
\section{Introduction}
A basic problem in data analysis is to arrange objects in linear order given
available data. For example, political scientists may want to order judges from
the most liberal to the most conservative person according to their rulings,
psychologists may try to order subjects by their developmental stage using
questionnaire data, or archaeologists may order graves according to age given the
found artifacts. This type of problem is 
sometimes referred to as \emph{ordination} usually called \emph{unidimensional 
seriation.} 

Seriation was first used as a formal method by \cite{seriation:Petrie:1899} to
find a chronological order for graves discovered in the Nile area given objects
found there. The idea was that different objects continuously go in and out
of fashion and a contingency table of grave sites times objects can be
rearranged using row and column permutations till all large values are close to
the diagonal. Initially, the rearrangement of rows and columns of the
contingency was done manually and the adequacy was only judged subjectively 
by the researcher. Later, \cite{seriation:Robinson:1951}, 
\cite{seriation:Kendall:1971} and others 
proposed measures of agreement between rows to quantify optimality of the 
resulting table. A comprehensive description of the development of seriation
in archeology can be found in \cite{seriation:Ihm:2005}.

Today \dots blabla \dots

\section{Unidimensional seriation}
To seriate a set of $n$ objects $\{O_1,\dots,O_n\}$ one typically starts with a
$n \times n$ symmetric matrix $\mathbf{S} = (s_{ij})$ where $s_{ij}$ represents
the proximity between the objects $O_i$ and $O_j$.  We assume that the
proximities increase with object similarity.  The problem is now to find a
permutation function $\pi: \{1,\dots,n\} \rightarrow \{1,\dots,n\}$, i.e. a
bijection that maps the set of indices of the objects (and equally of rows and
columns of $\mathbf{S}$) onto itself, which results in an order of objects most
consistent with the proximities in $\mathbf{S}$.
%The permutation function can also be used to relabel rows and columns
%of the proximity matrix resulting in a rearranged matrix 
%$\mathbf{R'} = [r(\pi(i),\pi(j))]$. 

A symmetric matrix where the values in all rows and columns only decrease when
moving away from the main diagonal is called a \emph{Robinson matrix} after the
statistician \cite{seriation:Robinson:1951}. Formally, 
a matrix $\mathbf{S}$ is in Robinson form if, and only if 
\begin{equation}
    s_{ij} \ge s_{i,j+1} \quad \text{for all} \quad j \ge i 
    \quad \text{and} \quad
    s_{i,j-1} \le s_{ij} \quad \text{for all} \quad j \le i
\end{equation}
holds. Since in such a matrix the largest values appear close to the main
diagonal, the closer objects are together in the order of the matrix,
the higher their proximity. This provides a natural way to define 
consistency and results in the
objective of seriation: find the permutation $\pi^*$ which
results in a rearranged proximity matrix $\mathbf{S'} = (s_{\pi(i)\pi(j)})$
which is in Robinson form. However, there are two problems:
\begin{enumerate}
    \item For most data sets, proximity matrices cannot be rearranged into a
        perfect Robinson form.
    \item The number of possible permutations is $n!$ which makes an exhaustive 
        search for medium to large $n$ infeasible.
\end{enumerate}

The first problem
indicates that one needs to find a permutation which results into a matrix as
close to the Robinson form as possible. To quantify closeness, measures of
divergence from the Robinson form have to be defined. Such measures are
defined in Section~\ref{sec:criteria}.

The second problem, to find the permutation that minimized the measure of
divergence from the Robinson form can be addressed by using search heuristics
or methods which reduce the search space (e.g., dynamic
programming~\citep{seriation:Hubert:1981}). Search heuristics for
different measures are presented in Section~\ref{sec:methods}.


\section{Seriation criteria}
\label{sec:criteria}
In the literature several criteria to judge the quality of a seriation are
suggested. In the following, we define some commonly used criteria.

Without loss of generality, we assume in the definitions that 
the proximity matrix contains dissimilarities (instead of similarities 
as used above).
This has just the technical reason that dissimilarity matrices are 
currently the most common from to store proximities in R (see \code{dist} in
package~\pkg{stats}). To reflect this change we will use the letter 
$\mathbf{D}$ for the $n \times n$ dissimilarity matrix. Typically,
similarities can be transformed into dissimilarities by
$\mathbf{D} = 1 / (1 + \mathbf{S})$ or by  $\mathbf{D} = 1 - \mathbf{S}$.


\subsection{Anti-Robinson events}

A Robinson matrix \citep{seriation:Robinson:1951} has its columns arranged such
that the values of the cells always decrease when moving away from the
diagonal. To measure the divergence from the Robinson form, the number of
anti-Robinson events (the value increases instead of decreases) can be counted.
The more anti-Robinson events occur in a matrix, the larger the divergence.
Note that we use a dissimilarity matrix instead of a similarity matrix used by
Robinson. Therefore cell values have to increase moving away from the main
diagonal to represent decreasing similarity.

Following~\cite{seriation:Chen:2002}, we define three criteria to measure the
divergence of a matrix from the Robinson form.  All three criteria use the
anti-Robinson events, but weight them differently:
\begin{equation}
    \sum_{i=1}^p \biggl( \sum_{j<k<i}^n I(d_{ij}<d_{ik})\; w_{ijk}%
    + \sum_{i<j<k} I(d_{ij} > d_{ik})\; w_{ijk} \biggr),
\end{equation}
where $I$ is an indicator function and $w_{ijk}$ is a weight.

For the raw number of anti-Robinson events, the weight is 
\begin{equation}
    w_{ijk} = 1.
\end{equation}

For the sum of absolute anti-Robinson deviations, the
deviations are used as the weight, i.e.,  
\begin{equation}
    w_{ijk} = |d_{ij} - d_{ik}|.
\end{equation}

For the weighted sum of absolute anti-Robinson deviations, the deviations are
weighted by the difference of column indices, i.e., 
\begin{equation}
    w_{ijk} = |j-k||d_{ij} - d_{ik}|.
\end{equation}

\subsection{Least squares criterion} 
A natural seriation criterion is to quantify the deviations between the 
dissimilarities in $\mathbf{D}$ and the rank differences of the objects
in the order $\pi$. Such deviations can be measured, e.g, by the
sum of squares of deviations \citep{seriation:Caraux:2005} defined by
\begin{equation}
    \sum_{i=1}^n \sum_{j=1}^n (d_{ij} - |i-j|)^2,
\end{equation}
where $|i-j|$ is the rank difference or gap between $O_i$ and $O_j$ in the 
order $\pi$.

The least squares criterion is related to 
unidimensional scaling~\citep{seriation:Leeuw:2005}, 
where the objective is to place all objects on a straight line such that
the dissimilarities in $\mathbf{D}$ are preserved 
by the relative positions in the best possible way.
The optimization problem of unidimensional scaling is to minimize 
$\sum_{i=1}^n \sum_{j=1}^n (d_{ij} - |x_i-x_j|)^2$ which is close
to the seriation problem, but in addition to the ranking of the 
objects also takes the distances between objects on the resulting
scale into account.

Note that if Euclidean distance is used to calculate $\mathbf{D}$ from a data
matrix $\mathbf{X}$, the order of the elements in $\mathbf{X}$ by projecting
them on the first principal component of $\mathbf{X}$ minimizes this criterion.
\marginpar{is that correct?}

\subsection{Inertia criterion} 
Another way to look at the seriation problem is not to 
focus on placing small dissimilarity values close to the diagonal, but to
push large values away from it. A measure to quantify this,
the moment of inertia of dissimilarity values 
around the diagonal \citep{seriation:Caraux:2005} is defined as
\begin{equation}
    \sum_{i=1}^n \sum_{j=1}^n d_{ij}|i-j|^2.
\end{equation}
$|i-j|$ is used as a measure for the distance to the diagonal and
$d_{ij}$ gives the weight. 
The criterion increases when higher dissimilarity values are placed
farther away from the diagonal.

\subsection{Hamiltonian path length}
The dissimilarity matrix $\mathbf{D}$ can be represented as a finite weighted
graph $G = (O,E)$ where the set of objects $O$ constitute the vertices and each
edge $e_{ij} \in E$ between the objects $O_i$ and $O_j$ has a weight $w_ij$
associated which represents the dissimilarity $d_{ij}$.

Such a graph can be used for 
seriation~\citep[see, e.g.,][]{seriation:Hubert:1974,seriation:Caraux:2005}. 
An order $\pi$ of the objects can be seen as a path through the graph where
each node is visited only once, i.e., a Hamilton path. The length of such a
path is given by
\begin{equation}
    \sum_{i=1}^{n-1} d_{\pi(i)\pi(i+1)}.
\end{equation}
Minimizing the path length results in an seriation optimal with respect to 
dissimilarities between neighboring objects. 
However, only dissimilarities adjacent to the main diagonal (i.e.,
between neighboring objects) contribute to the 
measure. Therefore, this criterion does not enforce a full Robinson form.

Note that the length of the Hamiltonian path is equal to the 
value of the \emph{minimal span loss function} 
\citep[as used by][]{seriation:Chen:2002}),
and both notions are related to the 
\emph{traveling salesperson problem}~\citep{seriation:Gutin:2002}.


\subsection{Measure of effectiveness (bond energy)}

\cite{seriation:McCormick:1972} defined the
the \emph{measure of effectiveness (ME)} for 
a $n \times m$ matrix $\mathbf{X} = (x_{ij})$ as
\begin{equation}
    \frac{1}{2}
    \sum_{i=1}^{n} \sum_{j=1}^{m} x_{ij}[x_{i,j-1}+x_{i,j+1}+
        x_{i-1,j}+x_{i+1,j}]
    \label{equ:ME}
\end{equation}
with, by convention $x_{0,j}=x_{n+1,j}=x_{i,0}=x_{i,m+1}=0$.
ME is maximized if each element is as closely related numerically
to its four neighboring elements.
\marginpar{check n and m for consistency}

%From the definition in (\ref{equ:BE}), it is easy to see that the
%measure can be decomposed into two independent terms, one only 
%dependent on the order of the columns (left product in the sum) and on of 
%the order of the rows (right product in the sum).

Although ME was developed for general matrices, it can be used as a measure of
seriation quality for symmetric similarity matrices since it gets maximal only
if all large values are grouped together around the main diagonal. 

\subsection{Stress}

Stress~\citep{seriation:Niermann:2005} measures
the conciseness of the presentation of a matrix/table and can
be seen as a purity function which compares the values in a matrix/table with
their neighbors. The stress measures used here 
are computed as the sum of squared
distances of each matrix entry from its adjacent entries. 
We define for a $n \times m$ matrix $\mathbf{X} = x_{ij}$
two types of neighborhoods:

\begin{itemize}
    \item The Moore neighborhood comprises the eight adjacent entries.
        The local stress measure for element $x_{ij}$ is defined as
        \begin{equation}
            stress_{ij} = \sum_{k=\max(1,i-1)}^{\min(n,i+1)} 
                \sum_{l=\max(1,j-1)}^{\min(m,j+1)} 
                (x_{ij} - x_{kl})^2
        \end{equation}

    \item The Neumann neighborhood comprises the four adjacent entries
        resulting in the local stress of $x_{ij}$ of
        \begin{equation}
            stress_{ij} = 
            \sum_{k=\max(1,i-1)}^{\min(n,i+1)} (x_{ij} - x_{kj})^2 + 
            \sum_{l=\max(1,j-1)}^{\min(m,j+1)} (x_{ij} - x_{il})^2
            %(x_{ij} - x(i-1,j))^2 + (x_{ij} - x(i+1,j))^2 +
            %(x_{ij} - x(i,j-1))^2 + (x_{ij} - x(i,j+1))^2
        \end{equation}
\end{itemize}

Both local stress measures can be used to construct measures for the whole
table by summing over all entries:
\begin{equation}
    \sum_{i=1}^n \sum_{j=1}^m stress_{ij}
\end{equation}

The major difference between the Moore and the Neumann neighborhood is that
for the later row and column permutations are separable and can
be optimized independently.

Obviously, stress can be also used to measure the quality of seriation for
symmetric proximity matrices, since it can only be optimal, if large values
are concentrated around the main diagonal.

\section{Seriation methods}
\label{sec:methods}


\subsection{Traveling salesperson problem solver}

Minimizing the length of a Hamiltonian path through a graph can be done by
solving a traveling salesperson problem.  The traveling salesperson or salesman
problem (TSP) is a well known and well researched combinatorial optimization
problem. The goal is to find the shortest tour that visits each city in a given
list exactly once and then returns to the starting city. In graph theory a TSP
tour is called a \emph{Hamiltonian cycle.} But for the seriation problem, we
are looking for a Hamiltonian path. \cite{seriation:Garfinkel:1985} described a
simple transformation of the TSP to find the shortest Hamiltonian path. To the
original $n \times n$ dissimilarity matrix $\mathbf{D}$ an additional row and
column of 0's is added (sometimes this is referred to as a \emph{dummy city}).
The solution of this $(n+1)$-city TSP, gives the shortest path where the city
representing the added row/column cuts the cycle into a linear path.

Despite this simple problem statement, solving the TSP is difficult since it
belongs to the class of NP-complete problems. In the seriation case with $n+1$
cities, $n!$ tours have to be checked. However, despite this vast searching
space, small instances can be solved efficiently using dynamic programming
\citep{seriation:Held:1962} and larger instances can be solved using
\emph{branch-and-cut} algorithms~\citep{seriation:Padberg:1990}. For large
instances or if running time is critical, a wide array of heuristics are
available, ranging from simple nearest neighbor approaches to construct a
tour~\citep{seriation:Rosenkrantz:1977} to complex heuristics like the
Lin-Kernighan heuristic~\citep{seriation:Lin:1973}.
A comprehensive overview can be found in \cite{seriation:Gutin:2002}.

\subsection{Hierarchical clustering}

Hierarchical clustering produces a series of nested clusterings which can be
visualized by a dendrogram, a binary tree where each internal node splits two
subtrees has a measure of similarity/dissimilarity attached to it.  A heuristic
to find a linear order of objects, the order of the leaf nodes in a dendrogram
structure can be used.  This idea is used, e.g.,  by heatmaps to reorder rows
and columns with the aim to place more similar objects and variables closer
together.  For hierarchical clustering several methods are available (e.g.,
single linkage, average linkage, complete linkage, ward method) resulting in
different dendrograms. 
However, the order of leaf nodes in a dendrogram is not unique.  A dendrogram
for $n$ objects has $2^{n-1}$ internal nodes (subtrees) and at  each internal
node the left and right subtree (or leaves) can be swapped resulting in
$2^{n-1}$ distinct leaf orderings.  

To find a unique or optimal order, an additional criterion
has to be defined.
\cite{seriation:Gruvaeus:1972} suggest to obtain a unique order by requiring to
order the leaf nodes such that at each level the objects at the edge of each
cluster are adjacent to that object outside the cluster to which it is nearest.

\cite{seriation:Bar-Joseph:2001} 
suggest to rearrange the dendrogram such that the 
Hamiltonian path connecting the leaves is minimized.
The  authors also present a algorithm with time complexity $O(n^4)$
to solve this optimization problem. Note that this problem is related to 
the TSP described above, however, the given dendrogram structure significantly
reduces the number of permissible permutations making the problem easier.


\subsection{Scaling}
MDS, PCA,\dots

\subsection{Rank-two ellipse seriation}

\cite{seriation:Chen:2002} proposes to
generate a sequence of correlation matrices
$R^1, R^2, \ldots$. $R^1$ is the correlation matrix
of the original distance matrix $\mathbf{D}$ and
\begin{equation}
R^{n+1} = \phi R^n,
\end{equation}
where $\phi(\cdot)$ calculates a correlation matrix.

The rank of the matrix $R^n$ falls with increasing $n$. The sequence
is continued till the first matrix in the sequence has a rank of 2.
Projecting all points in this matrix on the first two eigenvectors,
all points fall on an ellipse. The order of the points on this ellipse
is the resulting order where the ellipse can be cut at any of the 
two interception points (top or bottom) with the vertical axis.

\subsection{Bond energy algorithm}
BEA

Also talk about Murtagh's algorithm B.

\section{The \pkg{seriation} package infrastructure}

\begin{figure}[tp]
    \centerline{
    \includegraphics[width=12cm]{infrastructure}}

    \caption{The \pkg{seriation} package infrastructure}
    \label{fig:infrastructure}
\end{figure}

The basic infrastructure of the package is depicted in
Figure~\ref{fig:infrastructure}.
It consists of a method called \func{seriate} which takes 
objects of different R classes as input and returns an order
as an object of class \code{Order}. The class \code{Order} is virtual
has the following two concrete subclasses: 
\begin{itemize}
    \item \code{Order\_vector} which stores a single permutation vector to
        reorder, e.g., the objects in a dissimilarity matrix or the columns and
        rows of a symmetric matrix.  The class is implemented as a list with
        the element \code{order} containing the integer permutation vector.
    \item \code{Order\_matrix} which stores two permutation vectors, one
        for the columns and one for the rows of a matrix.
        The class is a list with the elements \code{row} and \code{column}.
\end{itemize}

For the special case of seriation of the leaf nodes of a dendrogram (class 
\code{hclust}) the resulting \code{Order\_vector} at the same time is also
a valid \code{hclust} object. This has the advantage, that all methods for
\code{hclust} (e.g., drawing a dendrogram) can still be used with the reordered
object.

For the subclasses of \code{Order} a simple construction function 
\code{Order(order, row, col)} is provided. Depending on whether 
\code{order} or \code{row} and \code{col} is given (each is a integer
permutation vector), the necessary subclass is created.

From Figure~\ref{fig:infrastructure} we see that the arguments of 
\func{seriate} are almost identical for different input data and
only vary slightly. For all input data the basic set of arguments is
the input data (\code{x}), the seriation method as a character string 
(\code{method}) and a list with optional control parameters for the 
seriation method (\code{control}). For \code{matrix} the additional argument
\code{margin} can be used if only column or row seriation is needed and not 
both. Is this the case, the result will be an object of class 
\code{Order\_vector} containing the respective order. For \code{hclust}
in second place the additional argument \code{dist} is inserted since
the dissimilarity information is needed by all seriation methods for
dendrogram reordering.


\begin{table}[t]
\centering
    \begin{tabular}{lccc}
        \hline
        Seriation method & Argument \code{method} & Input data \\
        \hline
        Traveling salesperson problem solver & \code{"tsp"} & \code{dist} \\
        Rank-two ellipse seriation & \code{"chen"} & \code{dist} \\
        Multidimensional scaling & \code{"mds"} & \code{dist} \\
        Bond Energy Algorithm & \code{"bea"} & \code{matrix} \\
        Murtagh's Algorithm B& \code{"murtagh"} & \code{matrix} \\
        Principal component analysis& \code{"pca"} & \code{matrix} \\
        Correspondence analysis& \code{"ca"} & \code{matrix} \\
        Optimal leaf ordering& \code{"optimal"} & \code{hclust}, \code{dist} \\
        Gruvaeus and Wainer& \code{"gw"} & \code{hclust}, \code{dist} \\
        \hline
    \end{tabular}
\caption{Implemented seriation methods.}
\label{tab:methods}
\end{table}

Seriation methods were already introduced in this paper in 
Section~\ref{sec:methods}. In Table~\ref{tab:methods} we summarize the
methods available for seriation.

To judge the quality of an order obtained by a seriation method, the 
method \code{criterion(x, order, method)} can be used to calculate different 
seriation criteria. \code{x} is the dissimilarity or data matrix, 
\code{order} contains a suitable object of class \code{Order} and
\code{method} specifies the criteria to calculate. We already defined
different seriation criteria in Section~\ref{sec:criteria}. In 
Table~\ref{tab:criteria} we summarize available criteria.

\begin{table}[t]
\centering
    \begin{tabular}{lccc}
        \hline
        Seriation criterion & Argument \code{method} & Input data \\
        \hline
        Hamiltonian path length & \code{"path\_length"} & \code{dist} \\
        Least squares criterion& \code{"least\_squares"} & \code{dist} \\
        Inertia criterion& \code{"inertia"} & \code{dist} \\
        Anti-Robinson events& \code{"ar\_i"}, \code{"ar\_s"}, \code{"ar\_w"} & \code{dist} \\
        Measure of effectiveness and bond energy& \code{"me"}, \code{"bond\_energy"} & \code{matrix} \\
        Stress (Neumann neighborhood)& \code{"neumann\_stress"} & \code{matrix} \\
        Stress (Moore neighborhood)& \code{"moore\_stress"} & \code{matrix} \\
        \hline
    \end{tabular}
\caption{Implemented seriation criteria.}
\label{tab:criteria}
\end{table}

In addition the package offers the convenience method \func{rearrange(x,
order)} to rearrange the objects in a dissimilarity matrix (\code{dist}) or
rows and columns of a matrix using a suitable \code{Order} object. 

For visualization, the package offers 
several options:
\begin{itemize}
\item Matrix shading with \func{pimage}.
\item Different heat maps (e.g., with optimally reordered
    dendrograms) with \func{hmap}.
\item Visualization of data matrices with \func{bertinplot}.
\item \emph{Dissimilarity plot}, a new visualization for cluster 
quality using matrix shading with \func{dissplot}.
\end{itemize}

\section{Examples and applications}
<<echo=FALSE>>=
options(scipen=3, digits=4)
@

\subsection{A first session using \pkg{seriation}}
In the following example, we use the well known iris data set which gives the
measurements in centimeters of the variables sepal length and width and petal
length and width, respectively, for 50 flowers from each of 3 species of iris
(Iris setosa, versicolor and virginica). 

First, we load the package \pkg{seriation} and the iris data set. We
remove the class attribute and reorder the objects randomly since they 
are sorted by species in the data set. Then we calculate the Euclidean
distances between the objects.

<<>>=
library("seriation")

data("iris")
x <- as.matrix(iris[-5])
x <- x[sample(1:nrow(x)),]
d <- dist(x)
@

To seriate the objects given the dissimilarities, we just call
\func{seriate} with the default settings.

<<>>=
order <- seriate(d)
order
@

The result is an object of class \code{Order\_vector} for the 
$150$ objects in the iris data set. The default method, a traveling
salesperson heuristic, was used.

To visually compare the random and the seriated data, 
we use matrix shading with \func{pimage}.

<<label=pimage1, eval=FALSE>>=
pimage(d, main = "Random")
pimage(d, order, main = "Reordered")
@
<<echo=FALSE, fig=FALSE, include=FALSE>>=
bitmap(file = "seriation-pimage1.png", type = "pnggray", 
    height = 2.8, width = 5, res = 300)
def.par <- par(no.readonly = TRUE)
layout(t(1:2))
<<pimage1>>
par(def.par)
tmp <- dev.off()
@
\begin{figure}
    \centering
    \includegraphics[width=12cm]{seriation-pimage1}
    \caption{Matrix shading of the  distance matrix for the iris data.}
    \label{fig:pimage1}
\end{figure}

Finally, we can also compare the values of different seriation 
criteria using \func{criterion}.

<<>>=
rbind(random = criterion(d), reordered = criterion(d, order))
@

Naturally, the reordered dissimilarity matrix achieves better values for all
criteria (note that for measure of effectiveness and inertia 
larger values are better).

Also the original data matrix can be easily inspected using \code{pimage}.
To rearrange the data matrix, a \code{Order\_matrix} is needed which
can be easily created by telling the construction \func{Order} 
to use \code{order} for row arrangement.

<<label=pimage2, eval=FALSE>>=
pimage(x, main = "Random")
pimage(x, Order(row = order), main = "Reordered")
@
<<echo=FALSE, fig=FALSE, include=FALSE>>=
bitmap(file = "seriation-pimage2.png", type = "pnggray", 
    height = 2.8, width = 5, res = 300)
def.par <- par(no.readonly = TRUE)
layout(t(1:2))
<<pimage2>>
par(def.par)
tmp <- dev.off()
@
\begin{figure}
    \centering
    \includegraphics[width=12cm]{seriation-pimage2}
    \caption{Matrix shading of the iris data matrix.}
    \label{fig:pimage2}
\end{figure}

\subsection{Comparing different seriation methods}

To compare different seriation methods we use again the randomized 
iris data set and the distance matrix from the previous example.

First, we use some seriation methods for dissimilarity matrices.

<<>>=
methods <- c("chen", "mds", "tsp")
order <- sapply(methods, simplify = FALSE, FUN = function(m) seriate(d, m))
@

Next, we add hierarchical clustering plus the implemented methods
for leaf node ordering.

<<>>=
hc <- Order(hclust(d, method = "average"))

order$hc <- hc
order$hc_gw <- seriate(hc, d, method = "gw")
order$hc_opt <- seriate(hc, d, method = "optimal")
#$
@

We use matrix shading to plot the resulting orderings 
(see Figure~\ref{fig:pimage3}).

<<label=pimage3, eval=FALSE>>=
tmp <- sapply(order, FUN = function(o) pimage(d, o, main = attr(o, "method")))
@
<<echo=FALSE, fig=FALSE, include=FALSE>>=
bitmap(file = "seriation-pimage3.png", type = "pnggray", height = 4.5, 
    width = 6, res = 300, pointsize = 9)
def.par <- par(no.readonly = TRUE)
layout(matrix(1:6, ncol = 3, byrow = TRUE))
<<pimage3>>
par(def.par)
tmp <- dev.off()
@

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{seriation-pimage3}
    \caption{Image plot the distance matrix for the iris data
    using rearrangement by different seriation methods.}
    \label{fig:pimage3}
\end{figure}

And finally, we compare the criterion values for the different seriation
methods. 
<<>>=
crit <- sapply(order, FUN = function(o) criterion(d, o))
crit
@

<<echo=FALSE, fig=TRUE, include=FALSE, label=crit1, width=6, height=10>>=
def.par <- par(no.readonly = TRUE)
m <- c("path_length", "least_squares", "ar_i", "moore_stress")
layout(matrix(1:length(m), ncol=1))
#tmp <- apply(crit[m,], 1, dotchart, sub =m)
tmp <- sapply(m, FUN = function(i) dotchart(crit[i,],sub = i))
par(def.par)
@

\begin{figure}
    \centering
    \includegraphics{seriation-crit1}
    \caption{Comparison of different methods and seriation criteria}
    \label{fig:crit1}
\end{figure}

For easier comparison, we plotted the results for the measures Hamiltonian
path length, least squares criterion, anti-Robinson events and stress using the
Moore neighborhood in Figure~\ref{fig:crit1}.  Clearly, the methods which
directly try to minimize the Hamiltonian path length (hierarchical clustering
with optimal leaf ordering and the TSP heuristic) provide the best results
concerning the path length.  For the least squares criterion and the
anti-Robinson events, using the first dimension of MDS provides the best
results.Regarding stress, TSP and MDS provide the best results.


\subsection{Heat maps}

\marginpar{Write something about heatmaps.}

We use again the randomly reordered iris data set \code{x} 
from the above example. To make the variables (columns) comparable,
we use standard scaling.

<<>>=
x <- scale(x, center = FALSE)
@

To produce a heat map with optimally reordered, the function \func{hmap} can be
used with its default settings. With this settings, the Euclidean distances
between rows and between columns are calculated (with \func{dist}),
hierarchical clustering (\func{hclust}) is performed, the resulting dendrograms
are optimally reordered, and \func{heatmap} in package \pkg{stats} is used
for plotting. 

<<eval=FALSE>>=
hmap(x)
hmap(x, dendrogram = FALSE)
@

If \code{dendrogram = FALSE} is used, instead of hierarchical clustering,
seriation on the dissimilarity matrices for rows and columns is performed
(per default a TSP heuristic)
and the reordered matrix with the reordered dissimilarity matrices to the left
and on top is displayed. A \code{method} argument can be used to choose 
different seriation methods.


<<echo=FALSE, fig=TRUE, include=FALSE, label=heatmap1>>=
hmap(x, cexCol=1, labRow = "")
@
<<echo=FALSE, fig=TRUE, include=FALSE, label=heatmap2>>=
hmap(x, dendrogram = FALSE)
@

\begin{figure}
    \begin{minipage}[b]{.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{seriation-heatmap1} \\
            (a)
    \end{minipage}
    \begin{minipage}[b]{.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{seriation-heatmap2} \\
            (b)
    \end{minipage}
    \caption{Two presentations of the rearranged iris data matrix. (a) as an
optimally reordered heat map and (b) as a seriated data matrix with reordered
dissimilarity matrices to the left and on top.}
    \label{fig:heatmap}
\end{figure}


\subsection{Bertin's permutation matrix}

\cite{seriation:Bertin:1999} introduced permutation matrices to analyze
multivariate data with medium to low sample size.  The idea is to reveal a more
homogeneous structure in a data matrix $\mathbf{X}$ by simultaneously
rearranging rows and columns. The rearranged matrix is displayed and cases and
variables can be grouped manually to gain a better understanding of the data.

To quantify homogeneity, a purity function
\begin{displaymath}
  \phi = \Phi(\mathbf{X})  
\end{displaymath}
is defined. Let $\Pi$ be the set of all permutation
$\pi$ for matrix $\mathbf{X}$.
Note that function $\pi$ performs row and column permutations on a matrix.
The optimal permutation with respect to
purity
\begin{displaymath}
  \pi^* = \argmax\nolimits_{\pi \in \Pi} \Phi(\pi(\mathbf{X}))  
\end{displaymath}
is found and the rearranged matrix $\pi(\mathbf{X})$ is
displayed. Since, depending on the purity function, finding the optimal
solution can be hard, often a near optimal solution is also acceptable.

A possible purity function $\Phi$ is:
Given distances between rows and columns of the data matrix, define purity as
the sum of distances of adjacent rows/columns.  Using this purity function,
finding the optimal permutation $\pi^*$ means solving two (independent) TSPs,
one for the columns and one for the rows.

As an example, we use the results of $8$ constitutional referenda for $41$
Irish communities~\citep{seriation:Falguerolles:1997}\footnote{The Irish data
set is included in this package. The original numbers and the text of the
referenda can be obtained from~\url{http://www.electionsireland.org/}}.  To
make values comparable across columns (variables), the ranks of the values in
each row are used instead of the original values.  

<<>>=
library("seriation")
data("Irish")

scale_by_rank <- function(x) apply(x, 2, rank)
orig_matrix <- scale_by_rank(Irish[,-6])
@

For seriation, we calculate distances between rows and between columns using
the sum of absolute rank differences (this is equal to the Minkowski distance
with power $1$). Then we apply seriation (using MDS) to both distance matrices
and create a \code{Order} object.  And plot the original and the reordered
matrix using \func{bertinplot}. 

<<>>=
order <- Order(
    row = seriate(dist(orig_matrix, "minkowski", p = 1), method=  "tsp"),
    col = seriate(dist(t(orig_matrix), "minkowski", p = 1), method = "tsp")
)
@



<<eval=FALSE>>=
bertinplot(orig_matrix)
bertinplot(orig_matrix, order)
@
<<echo=FALSE, fig=TRUE, include=FALSE, label=bertin1, width=10>>=
bertinplot(orig_matrix)
@
<<echo=FALSE, fig=TRUE, include=FALSE, label=bertin2, width=10>>=
bertinplot(orig_matrix, order)
@
\begin{figure}
    \centering
    \includegraphics[width=16cm, trim=0 60 0 0]{seriation-bertin1} \\
    (a)
    
    \includegraphics[width=16cm, trim=0 60 0 0]{seriation-bertin2} \\
    (b)    
    \caption{Bertin plot for the (a) original arrangement and the (b) 
    reordered Irish data set.}
    \label{fig:bertin}
\end{figure}



The original matrix and the rearranged matrix are shown in
Figure~\ref{fig:bertin} as a matrix of bars where high values are highlighted
(filled blocks).  Note that following Bertin, the cases (communities) are
displayed as the columns and the variables (referenda) as rows.  Depending on
the number of cases and variables, columns and rows can be exchanged to obtain
a better visualization.

Although the columns are already ordered (communities in the same city appear
consecutively) in the original data matrix in Figure~\ref{fig:bertin}(a), it
takes some effort to find structure in the data.  For example, it seems that
the variables `Marriage', `Divorce', `Right to Travel' and `Right to
Information' are correlated since the values are all high in the block made up
by the columns of the communities in Dublin.  The reordered matrix affirm this
but makes the structure much more apparent. Especially the contribution of low
values (which are not highlighted) to the overall structure becomes only
visible after rearrangement.

\subsection{Binary data matrices}

\marginpar{Write about incidence matrices.}
\marginpar{Write about townships.}



<<>>=
library("seriation")
data("Townships")
@

<<fig=TRUE, include=FALSE, label=binary1, width=9>>=
bertinplot(Townships, options = list(panel=panel.squares, spacing = 0))
@


To get a good solution, we repeat BEA $10$ times and take the best solution.

<<fig=TRUE, include=FALSE, label=binary2, width=9>>=
order <- seriate(Townships, method = "bea", control = list(rep = 10))
bertinplot(Townships,order, options = list(panel=panel.squares, spacing = 0))
@

\begin{figure}
    \centering
    \includegraphics[width=10cm]{seriation-binary1} \\
    (a)    

    \includegraphics[width=10cm]{seriation-binary2} \\
    (b)   

    \caption{Original and reordered (BEA) townships data set.}
    \label{fig:heatmap}
\end{figure}


<<>>=
rbind(original = criterion(Townships), reordered = criterion(Townships, order))
@

\subsection{Dissimilarity plot}

<<>>=
library("seriation")
data("iris")
d <- dist(iris[-5])
@

<<fig=TRUE, include=FALSE, label=dissplot1>>=
## plot original matrix
res <- dissplot(d, method = NA)
@

<<fig=TRUE, include=FALSE, label=dissplot2>>=
## plot reordered matrix using the nearest insertion algorithm (from tsp)
res <- dissplot(d, method = "tsp", control = list(method = "nearest"),
    options = list(main = "Seriation (TSP)"))
@

\begin{figure}
    \begin{minipage}[b]{.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{seriation-dissplot1} \\
    (a)    
    \end{minipage}
    \begin{minipage}[b]{.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{seriation-dissplot2} \\
    (b)   
    \end{minipage}
    \caption{Two dissimilarity plots. 
    (a) the original dissimilarity matrix and 
    (b) the seriated dissimilarity matrix.}
    \label{fig:dissplot1}
\end{figure}


<<>>=
## cluster with pam (we know iris has 3 clusters)
library("cluster")
l <- pam(d, 3, cluster.only = TRUE)
@

<<fig=TRUE, include=FALSE, label=dissplot3>>=
## we use a grid layout to place several plots on a page
grid.newpage()
pushViewport(viewport(layout=grid.layout(nrow = 2, ncol = 2), 
    gp = gpar(fontsize = 8)))
pushViewport(viewport(layout.pos.row = 1, layout.pos.col = 1))

## visualize the clustering
res <- dissplot(d, l, method = "chen",  
    options = list(main = "PAM + Seriation (Chen) - standard", 
    newpage = FALSE))

popViewport()
pushViewport(viewport(layout.pos.row = 1, layout.pos.col = 2))

## supress average in lower triangle
plot(res, options = list(main = "PAM + Seriation (Chen) - no avg.", 
    average = FALSE, newpage = FALSE))


popViewport()
pushViewport(viewport(layout.pos.row = 2, layout.pos.col = 1))

## color: use 5 shades of blue
plot(res, options = list(main = "PAM + Seriation (Chen) - blue", 
    col = hcl(h = 260, c = seq(75,0, length=5), l = seq(30,95, length=5)),
    gp = gpar(fill = hcl(h = 260, c=30, l = 80)), newpage = FALSE))

popViewport()
pushViewport(viewport(layout.pos.row = 2, layout.pos.col = 2))

## threshold
plot(res, options = list(main = "PAM + Seriation (Chen) - threshold", 
    threshold = 1.5, newpage = FALSE))

popViewport(2)

## the cluster_dissimilarity_matrix object
res 
names(res)
@

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{seriation-dissplot3}
    \caption{Dissimilarity plot for pam solution with $k=3$ using several
    presentation options.}
    \label{fig:dissplot3}
\end{figure}


\section{Conclusion}

%\section*{Acknowledgments}

%
\bibliographystyle{abbrvnat}
\bibliography{seriation}
%
\end{document}

